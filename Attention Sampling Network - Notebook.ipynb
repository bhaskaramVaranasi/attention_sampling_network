{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWaNJOsxsFU3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "6c14f7bc-2aa5-4d97-8834-98736ff82159"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8rm8IcjsHss",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8c5fcbf1-fb4c-456f-8418-5e9e8fdc156f"
      },
      "source": [
        "cd '/content/drive/My Drive/ComputerVision_project'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/ComputerVision_project\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wm3CTC_BsQ0x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f16a8d05-fc3e-446d-dfc2-ee81d9871ce6"
      },
      "source": [
        "from __future__ import print_function, division\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "from skimage import io, transform\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "\n",
        "import json\n",
        "from os import path\n",
        "\n",
        "from skimage.io import imsave\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "import torch\n",
        "from skimage import io, transform\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torch\n",
        " \n",
        "from torch.distributions.multinomial import Multinomial\n",
        "from torch.distributions.categorical import Categorical\n",
        "from torch.distributions.uniform import Uniform\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Batcher():\n",
        "    \"\"\"Assemble a sequence of things into a sequence of batches.\"\"\"\n",
        "    def __init__(self, sequence, batch_size=16):\n",
        "        self._batch_size = batch_size\n",
        "        self._sequence = sequence\n",
        "        self._idxs = np.arange(len(self._sequence))\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self._sequence) / self._batch_size))\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        if i >= len(self):\n",
        "            raise IndexError(\"Index out of bounds\")\n",
        "\n",
        "        start = i*self._batch_size\n",
        "        end = min(len(self._sequence), start+self._batch_size)\n",
        "        data = [self._sequence[j] for j in self._idxs[start:end]]\n",
        "        inputs = [d[0] for d in data]\n",
        "        outputs = [d[1] for d in data]\n",
        "\n",
        "        return self._stack(inputs), self._stack(outputs)\n",
        "\n",
        "    def _stack(self, data):\n",
        "        if data is None:\n",
        "            return None\n",
        "\n",
        "        if not isinstance(data[0], (list, tuple)):\n",
        "            return np.stack(data)\n",
        "\n",
        "        seq = type(data[0])\n",
        "        K = len(data[0])\n",
        "        data = seq(\n",
        "            np.stack([d[k] for d in data])\n",
        "            for k in range(K)\n",
        "        )\n",
        "\n",
        "        return data\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        np.random.shuffle(self._idxs)\n",
        "        self._sequence.on_epoch_end()\n",
        "\n",
        "\n",
        "class MNIST():\n",
        "    \"\"\"Load a Megapixel MNIST dataset.\"\"\"\n",
        "    def __init__(self, dataset_dir, train=True):\n",
        "        \"\"\"Passing the directory name to the JSON parameters, to load th parameters.\"\"\"\n",
        "        with open(path.join(dataset_dir, \"parameters.json\")) as f:\n",
        "            self.parameters = json.load(f)\n",
        "\n",
        "        filename = \"train.npy\" if train else \"test.npy\"\n",
        "        N = self.parameters[\"n_train\" if train else \"n_test\"]\n",
        "        W = self.parameters[\"width\"]\n",
        "        H = self.parameters[\"height\"]\n",
        "        scale = self.parameters[\"scale\"]\n",
        "\n",
        "        self._high_shape = (H, W, 1)\n",
        "        self._low_shape = (int(scale*H), int(scale*W), 1)\n",
        "        self._data = np.load(path.join(dataset_dir, filename),allow_pickle=True)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._data)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        if i >= len(self):\n",
        "            raise IndexError()\n",
        "\n",
        "        # Placeholders\n",
        "        x_low = np.zeros(self._low_shape, dtype=np.float32).ravel()\n",
        "        x_high = np.zeros(self._high_shape, dtype=np.float32).ravel()\n",
        "\n",
        "        # Fill the sparse representations\n",
        "        data = self._data[i]\n",
        "        x_low[data[0][0]] = data[0][1]\n",
        "        x_high[data[1][0]] = data[1][1]\n",
        "\n",
        "        # Reshape to their final shape\n",
        "        x_low = x_low.reshape(self._low_shape)\n",
        "        x_high = x_high.reshape(self._high_shape)\n",
        "\n",
        "        return [x_low, x_high], data[2]\n",
        "\n",
        "\n",
        "class AttentionSaver():\n",
        "    def __init__(self, output, att_model, data):\n",
        "        self._att_path = path.join(output, \"attention_{:03d}.png\")\n",
        "        self._patches_path = path.join(output, \"patches_{:03d}_{:03d}.png\")\n",
        "        self._att_model = att_model\n",
        "        (self._x, self._x_high), self._y = data[0]\n",
        "        self._imsave(\n",
        "            path.join(output, \"image.png\"),\n",
        "            self._x[0, :, :, 0]\n",
        "        )\n",
        "\n",
        "    def on_epoch_end(self, e, logs):\n",
        "        att, patches = self._att_model.predict([self._x, self._x_high])\n",
        "        self._imsave(self._att_path.format(e), att[0])\n",
        "        np.save(self._att_path.format(e)[:-4], att[0])\n",
        "        for i, p in enumerate(patches[0]):\n",
        "            self._imsave(self._patches_path.format(e, i), p[:, :, 0])\n",
        "\n",
        "    def _imsave(self, filepath, x):\n",
        "        x = (x*65535).astype(np.uint16)\n",
        "        imsave(filepath, x, check_contrast=False)\n",
        "\n",
        "class MegaMnistDataset(Dataset):\n",
        "    \"\"\"Face Landmarks dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, dataset_dir, train ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            csv_file (string): Path to the csv file with annotations.\n",
        "            root_dir (string): Directory with all the images.\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        \"\"\"\n",
        "        self.mnist = MNIST(dataset_dir=dataset_dir, train=train)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.mnist)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        [x_low, x_high], response = self.mnist[idx]\n",
        "        x_low, x_high = np.transpose(x_low, (2,0,1)), np.transpose(x_high, (2,0,1))\n",
        "        x_low = torch.Tensor(x_low)\n",
        "        x_high = torch.Tensor(x_high)\n",
        "        response = torch.Tensor(response)\n",
        "        return [x_low, x_high], response\n",
        "\n",
        "class SampleSoftmaxLayer(nn.Module):\n",
        "    def __init__(self, squeeze_channels = False, smooth=0, **kwargs):\n",
        "        self.squeeze_channels = squeeze_channels\n",
        "        self.smooth = torch.tensor(smooth, device = device, requires_grad=True)\n",
        "        super(SampleSoftmaxLayer, self).__init__()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Apply softmax to the whole x (per sample)\n",
        "        input_shape = x.shape\n",
        "        x = x.view(input_shape[0], -1)\n",
        "        x = F.softmax(x)\n",
        "        # smooth the distribution ? what distribution\n",
        "        if 0 < self.smooth < 1:\n",
        "            x = x * ( 1 - self.smooth)\n",
        "            x = x + self.smooth / torch.tensor(x.shape[1], device = device, dtype= torch.float32, requires_grad= True)\n",
        "\n",
        "        # Finally reshape to the original shape\n",
        "        x = x.view(input_shape)\n",
        "\n",
        "        if self.squeeze_channels:\n",
        "            # channels are at the beginning now\n",
        "            channels = 1\n",
        "            x = torch.squeeze(x, channels)\n",
        "        return x\n",
        "\n",
        "class L2NormalizeLayer(nn.Module):\n",
        "    def __init__(self, axis = -1, **kwargs):\n",
        "        self.axis = axis\n",
        "        super(L2NormalizeLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # https://discuss.pytorch.org/t/how-to-normalize-embedding-vectors/1209/7\n",
        "        norm = x.norm(p = 2, dim = self.axis, keepdim= True)\n",
        "        response =  x.div(norm).detach()\n",
        "        response[torch.isnan(response)] = 0\n",
        "        response[torch.isinf(response)] = 0\n",
        "        return response\n",
        "\n",
        "def _sample_with_replacement(logits):\n",
        "    \"\"\"Sample with replacement using the tensorflow op.\"\"\"\n",
        "    return Categorical(logits=logits)\n",
        " \n",
        "def _sample_without_replacement(logits, n_samples):\n",
        "    \"\"\"Sample without replacement using the Gumbel-max trick.\n",
        " \n",
        "    See lips.cs.princeton.edu/the-gumbel-max-trick-for-discrete-distributions/\n",
        "    \"\"\"\n",
        "    z = -torch.log(-torch.log(torch.rand(logits.shape, device = device)))\n",
        "    # returns the indices of the top n samples\n",
        "    return torch.topk(logits+z, k=n_samples, dim=1)[1]\n",
        " \n",
        "def gather_nd(params, indices, name=None):\n",
        "    '''\n",
        "    the input indices must be a 2d tensor in the form of [[a,b,..,c],...], \n",
        "    which represents the location of the elements.\n",
        "    '''\n",
        "    \n",
        "    indices = indices.t().long()\n",
        "    ndim = indices.size(0)\n",
        "    idx = torch.zeros_like(indices[0], device = device, requires_grad = True).long()\n",
        "    m = 1\n",
        "    \n",
        "    for i in range(ndim)[::-1]:\n",
        "        idx += indices[i] * m \n",
        "        m *= params.size(i)\n",
        "    \n",
        "    return torch.take(params, idx, )\n",
        " \n",
        "def unravel_index(index, shape):\n",
        "    #return torch.tensor(np.unravel_index(index.cpu().numpy(), shape), device = device)\n",
        "    \n",
        "    out = []\n",
        "    for dim in reversed(shape):\n",
        "        out.append(index % dim)\n",
        "        index = index // dim\n",
        "    return torch.cat(out[::-1], 0)\n",
        " \n",
        "def sample(n_samples, attention, sample_space, replace=False,\n",
        "           use_logits=False):\n",
        "    \"\"\"Sample from the passed in attention distribution.\n",
        " \n",
        "    Arguments\n",
        "    ---------\n",
        "        n_samples: int, the number of samples per datapoint\n",
        "        attention: tensor, the attention distribution per datapoint (could be\n",
        "                   logits or normalized)\n",
        "        sample_space: This should always equal K.shape(attention)[1:]\n",
        "        replace: bool, sample with replacement if set to True (defaults to\n",
        "                 False)\n",
        "        use_logits: bool, assume the input is logits if set to True (defaults\n",
        "                    to False)\n",
        "    \"\"\"\n",
        "    \n",
        "    # Make sure we have logits and choose replacement or not\n",
        "    input_shape = attention.shape\n",
        "    logits = attention if use_logits else torch.log(attention)\n",
        "    sampling_function = (\n",
        "        _sample_with_replacement if replace\n",
        "        else _sample_without_replacement\n",
        "    )\n",
        "    \n",
        "    # Flatten the attention distribution and sample from it\n",
        "    logits = logits.view(-1, torch.prod(torch.tensor(sample_space)))\n",
        "    samples = sampling_function(logits, n_samples)\n",
        "    good_samples = samples\n",
        "    \n",
        " \n",
        "    \n",
        "    # Unravel the indices into sample_space\n",
        "    batch_size = attention.shape[0]\n",
        "    n_dims = torch.tensor(sample_space).shape[0]\n",
        "    samples =  unravel_index(samples.view((-1,)), sample_space)\n",
        "    \n",
        "    samples = torch.t(samples).view((batch_size, n_samples, n_dims))\n",
        " \n",
        "    # Concatenate with the indices into the batch dimension in order to gather\n",
        "    # the attention values\n",
        "    batch_indices = (\n",
        "        torch.arange(0,batch_size, device=device).view((-1, 1, 1)) *\n",
        "        torch.ones((1, n_samples, 1), device=device)\n",
        "    )\n",
        "    \n",
        "    indices = torch.cat([batch_indices.long(), samples.long()], -1)\n",
        "    \n",
        "    wow = attention.view(input_shape[0], 174 * 174)\n",
        "    sampled_attention = torch.gather(wow, 1, good_samples)\n",
        " \n",
        "    # Gather the attention\n",
        "    #sampled_attention = attention.masked_select(indices)\n",
        " \n",
        "    return samples, sampled_attention\n",
        "\n",
        "\n",
        "class AttentionModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(AttentionModel, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(1, 8, kernel_size =3,)\n",
        "    self.conv2 = nn.Conv2d(8,8, kernel_size = 3, )\n",
        "    self.conv3 = nn.Conv2d(8, 1, kernel_size=3, )\n",
        "    self.sample_softmax = SampleSoftmaxLayer(squeeze_channels=True, smooth=1e-5)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = torch.tanh(self.conv1(x))\n",
        "    x = torch.tanh(self.conv2(x))\n",
        "    x = self.conv3(x)\n",
        "    response =  self.sample_softmax(x)\n",
        "    return response\n",
        "\n",
        "class FeatureModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(FeatureModel, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(1, 32 ,kernel_size=7)\n",
        "    self.conv2 = nn.Conv2d(32, 32 ,kernel_size=3)\n",
        "    self.conv3 = nn.Conv2d(32, 32 ,kernel_size=3)\n",
        "    self.conv4 = nn.Conv2d(32, 32 ,kernel_size=3)\n",
        "    self.avg_pooling = nn.AdaptiveMaxPool2d(1)\n",
        "    self.l2_layer = nn.BatchNorm2d(32)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = torch.relu(self.conv1(x))\n",
        "    x = torch.relu(self.conv2(x))\n",
        "    x = torch.relu(self.conv3(x))\n",
        "    x = torch.relu(self.conv4(x))\n",
        "    x = self.avg_pooling(x)\n",
        "    return self.l2_layer(x)\n",
        "\n",
        "\n",
        "class SamplePatches(nn.Module):\n",
        "    def __init__(self,n_patches, patch_size, receptive_field =0, replace= False, use_logits = False, **kwargs):\n",
        "        self._n_patches = n_patches\n",
        "        self._patch_size = patch_size\n",
        "        self._receptive_field = receptive_field\n",
        "        self._replace = replace\n",
        "        self._use_logits = use_logits\n",
        "\n",
        "        super(SamplePatches, self).__init__()\n",
        "    \n",
        "    def forward(self, x_low, x_high, attention):\n",
        "        sample_space = attention.shape[1:]\n",
        "        samples, sampled_attention = sample(\n",
        "            self._n_patches,\n",
        "            attention,\n",
        "            sample_space,\n",
        "            replace=self._replace,\n",
        "            use_logits= self._use_logits\n",
        "        )\n",
        "\n",
        "        offsets = torch.zeros_like(samples, device = device, dtype=torch.float32, requires_grad = True)\n",
        "        if self._receptive_field > 0:\n",
        "            offsets = offsets + self._receptive_field / 2\n",
        "        patches, _ = FromTensors([x_low, x_high], None).patches(\n",
        "            samples,\n",
        "            offsets,\n",
        "            sample_space,\n",
        "            torch.tensor(x_low.shape[2:]) - self._receptive_field,\n",
        "            self._patch_size,\n",
        "            0,\n",
        "            1\n",
        "        )\n",
        "        return [patches, sampled_attention]\n",
        "\n",
        "\n",
        "class TotalReshape(nn.Module):\n",
        "  def __init__(self, shape):\n",
        "    self._shape = shape\n",
        "  \n",
        "  def forward(self, x):\n",
        "    return x.view(self._shape)\n",
        "\n",
        "\n",
        "def expand_many(x, axes):\n",
        "  for ax in axes:\n",
        "    x = x.unsqueeze(ax)\n",
        "  return x\n",
        "\n",
        "class FromTensors():\n",
        "  def __init__(self, xs, y):\n",
        "    self._xs = xs\n",
        "    self._y = y\n",
        "  \n",
        "  def targets(self):\n",
        "    return self._y\n",
        "  \n",
        "  def inputs(self):\n",
        "    return []\n",
        "\n",
        "  def data(self, level):\n",
        "    return self._xs[level]\n",
        "\n",
        "  def _scale(self, shape_from, shape_to):\n",
        "    shape_from = shape_from\n",
        "    shape_to = shape_to\n",
        "    return shape_to / shape_from\n",
        "  \n",
        "  def _shape(self, level):\n",
        "    x = self._xs[level]\n",
        "    int_shape = x.shape[2:]\n",
        "    \n",
        "    if not any(s is None for s in int_shape):\n",
        "      return int_shape\n",
        "    return x.shape[2:]\n",
        "\n",
        "  def patches(self, samples, offsets, sample_space, previous_patch_size, patch_size, fromlevel, tolevel):\n",
        "    # Make sure that everything is a pixel\n",
        "    sample_space = torch.tensor(sample_space, device = device, dtype=torch.float32, requires_grad=True)\n",
        "    previous_patch_size = torch.tensor(previous_patch_size, device = device, dtype=torch.float32, requires_grad=True)\n",
        "    patch_size = torch.tensor(patch_size, device = device, dtype=torch.float32, requires_grad=True)  \n",
        "    shape_from = torch.tensor(self._shape(fromlevel), device = device, dtype=torch.float32, requires_grad=True)\n",
        "    shape_to = torch.tensor(self._shape(tolevel), device = device, dtype=torch.float32, requires_grad=True)\n",
        "\n",
        "    # compute the scales\n",
        "    scale_samples = self._scale(sample_space, shape_to)\n",
        "    scale_offsets = self._scale(shape_from, shape_to)\n",
        "    # compute the scales\n",
        "    space_available = previous_patch_size  * scale_offsets\n",
        "    steps = space_available / sample_space\n",
        "\n",
        "    # steps is the offset per pixel \n",
        "    offsets = (\n",
        "      offsets.float() * expand_many(scale_offsets.float(), [0, 0]) +\n",
        "      samples.float() * expand_many(steps, [0, 0]) +\n",
        "      expand_many(steps / torch.tensor(2, device = device, dtype=torch.float32, requires_grad=True), [0, 0]) -\n",
        "      expand_many(patch_size / torch.tensor(2, device = device, dtype=torch.float32, requires_grad=True), [0, 0])\n",
        "    )\n",
        "\n",
        "    patches = extract_patches(\n",
        "        self._xs[tolevel],\n",
        "        offsets,\n",
        "        patch_size\n",
        "    )\n",
        "\n",
        "    return patches, offsets\n",
        "\n",
        "class SamplingWithReplacement(torch.autograd.Function):\n",
        "  @staticmethod\n",
        "  def forward(ctx, weights, attention, features):\n",
        "    ctx.save_for_backward(weights, attention, features)\n",
        "    wf = expand_many(weights, [-1] * (features.ndim - 2) )\n",
        "    F = torch.sum(wf * features, axis = 1)\n",
        "    return F\n",
        "  \n",
        "  @staticmethod\n",
        "  def backward(ctx, grad_output):\n",
        "    weights, attention, features = ctx.saved_tensors\n",
        "    grad_output.unsqueeze_(-1)\n",
        "    \n",
        "    # gradient of attention\n",
        "    ga = grad_output * features\n",
        "    ga = torch.sum(ga, axis = list(range(2, torch.ndim(ga))))\n",
        "    ga = ga * weights / attention\n",
        "    \n",
        "    # geadient of features\n",
        "    gf = wf * grad_output\n",
        "    return [None, ga, gf]\n",
        "\n",
        "def _SamplingWithoutReplacement(weights, attention, features):\n",
        "  axes = [-1] * ((features).ndimension() - 2 )\n",
        "  wf = expand_many(weights, axes)\n",
        "  af = expand_many(attention, axes)\n",
        "  pm = 1 - torch.cumsum(attention, 1)\n",
        "  pmf = expand_many(pm, axes)\n",
        "  Fa = af * features\n",
        "  Fpm = pmf * features\n",
        "  Fa_cumsum = torch.cumsum(Fa, axis = 1)\n",
        "  F_estimater = Fa_cumsum + Fpm\n",
        "  F = torch.sum(wf * F_estimater, axis =1)\n",
        "  return F\n",
        "\n",
        "class SamplingWithoutReplacement(torch.autograd.Function):\n",
        "  @staticmethod\n",
        "  def forward(ctx, weights, attention, features):\n",
        "    ctx.save_for_backward(weights, attention, features)\n",
        "\n",
        "    axes = [-1] * ((features).ndimension() - 2 )\n",
        "    wf = expand_many(weights, axes)\n",
        "    af = expand_many(attention, axes)\n",
        "    pm = 1 - torch.cumsum(attention, 1)\n",
        "    pmf = expand_many(pm, axes)\n",
        "    Fa = af * features\n",
        "    Fpm = pmf * features\n",
        "    Fa_cumsum = torch.cumsum(Fa, axis = 1)\n",
        "    F_estimater = Fa_cumsum + Fpm\n",
        "    F = torch.sum(wf * F_estimater, axis =1)\n",
        "    return F\n",
        "  \n",
        "  @staticmethod\n",
        "  def backward(ctx, grad_output):\n",
        "    weights, attention, features = ctx.saved_tensors\n",
        "    axes = [-1] * ((features).ndimension() - 2 )\n",
        "    af = expand_many(attention, axes)\n",
        "    wf = expand_many(weights, axes)\n",
        "    pm = 1 - torch.cumsum(attention, 1)\n",
        "    pmf = expand_many(pm, axes)\n",
        "    Fa = af * features\n",
        "    Fpm = pmf * features\n",
        "    Fa_cumsum = torch.cumsum(Fa, axis = 1)\n",
        "    F_estimator = Fa_cumsum + Fpm\n",
        "    F = torch.sum(wf * F_estimator, axis =1)\n",
        "\n",
        "    axes = [-1] * ((features).ndimension() - 2 )\n",
        "    N = attention.shape[1]\n",
        "    probs = attention / pm\n",
        "    probsf = expand_many(probs, axes)\n",
        "    grad_output = torch.unsqueeze(grad_output, 1)\n",
        "    ga1 = F_estimator / probsf\n",
        "    ga2 = (torch.cumsum(features, axis =1) - expand_many(torch.arange(N, device=device, dtype=torch.float32), [0] + axes) * features )\n",
        "    ga = grad_output * (ga1 + ga2)\n",
        "    ga = ga.sum(axis = list(range(2, ga.ndimension())))\n",
        "    ga = ga * weights\n",
        "\n",
        "    gf = expand_many(torch.arange(N-1,-1,-1, device=device).float(), [0] + axes)\n",
        "    gf = pmf + gf * af\n",
        "    gf = wf * gf\n",
        "    gf = gf * grad_output\n",
        "\n",
        "    return None, ga, gf\n",
        "\n",
        "def expected(attention, features, replace = False, weights = None):\n",
        "  if weights is None:\n",
        "    weights = torch.ones_like(attention) / torch.tensor(attention.shape[1], dtype=torch.float32)\n",
        "  \n",
        "  E = SamplingWithReplacement.apply if replace else SamplingWithoutReplacement.apply\n",
        "  return E(weights, attention, features)\n",
        "\n",
        "class BhasRegularizer(nn.Module):\n",
        "  def __init__(self, strength, eps):\n",
        "    super(BhasRegularizer, self).__init__()\n",
        "    self.strength = torch.tensor(strength, dtype = torch.float32, device= device, requires_grad=True)\n",
        "    self.eps = torch.tensor(eps, dtype = torch.float32, device= device, requires_grad=True)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    logx = torch.log(x + self.eps)\n",
        "    return self.strength * torch.sum(x*logx) / torch.tensor(x.shape[0], dtype = torch.float32, device= device, requires_grad=True)\n",
        "\n",
        "class ExpectationLayer(nn.Module):\n",
        "  def __init__(self, replace = False):\n",
        "    self._replace = replace\n",
        "    super(ExpectationLayer, self).__init__()\n",
        "\n",
        "  def forward(self, x):\n",
        "    features, sampled_attention = x\n",
        "    return expected(sampled_attention, features, replace = self._replace)\n",
        "\n",
        "\n",
        "class AttentionSamplingModel(nn.Module):\n",
        "  def __init__(self, output_size, patch_size, n_patches, replace= False, attention_regularizer=None, receptive_field = 0):\n",
        "    super(AttentionSamplingModel, self).__init__()\n",
        "    channels = dims = 1\n",
        "    # other hyper params used in the network\n",
        "    self.patch_size = patch_size\n",
        "    self.n_patches = n_patches\n",
        "    self.replace = replace\n",
        "    self.attention_regularizer = BhasRegularizer(1,0.0001)\n",
        "    # defining layers of repsonse\n",
        "    self.attention = AttentionModel()\n",
        "    self.feature = FeatureModel()\n",
        "    self.layer = nn.Linear(32,output_size)\n",
        "    self.samplePatchesLayer = SamplePatches( patch_size=patch_size, n_patches=n_patches, replace= False, attention_regularizer=None, receptive_field = 0)\n",
        "    self.totalReshape1 = ((-1, channels, *patch_size,))\n",
        "    self.totalReshape2 = ((-1, patch_size, dims))\n",
        "    self.expectation = ExpectationLayer(replace )\n",
        "    # make sure certain params are filled\n",
        "    if receptive_field is None:\n",
        "        raise NotImplementedError((\"Receptive field is not implemented yet\"))\n",
        "    # make sure that patch values are filled\n",
        "    if patch_size is None:\n",
        "        # we need the patch size to be present\n",
        "        # if needed we will add code to cover this functionality\n",
        "        raise NotImplementedError(\"Patch size cannot be None\")\n",
        "  \n",
        "  def forward(self, x):\n",
        "    # we expect that the input values will be a list of items\n",
        "    x_low, x_high = x\n",
        "    attention_map = self.attention(x_low)\n",
        "\n",
        "    # place holder to add attention regularizer\n",
        "    patches, sampled_attention = self.samplePatchesLayer(x_low, x_high, attention_map)\n",
        "    # compute features of the sampled patches\n",
        "    channels = patches.shape[-1]\n",
        "    #x.view(self._shape)\n",
        "    patches_flat = patches.view(self.totalReshape1)\n",
        "    patches_flat = patches_flat.clone().detach().requires_grad_(True)\n",
        "    #print('Patches flat', patches_flat, torch.isnan(patches_flat).any())\n",
        "    patch_features = self.feature(patches_flat)\n",
        "    dims = patch_features.shape[-3]\n",
        "    patch_features = patch_features.view((-1, self.n_patches, dims))\n",
        "    #print('Patch features ', patch_features)\n",
        "    sample_features = self.expectation([patch_features, sampled_attention])\n",
        "    #print('sample features', sample_features)\n",
        "    response =  F.softmax(self.layer(sample_features))\n",
        "    return response, self.attention_regularizer(attention_map)\n",
        "    return [sample_features, attention_map, patches]\n",
        "\n",
        "def _p(x, name=None):\n",
        "    shape = None\n",
        "    if x.name:\n",
        "        name = x.name\n",
        "    if x.shape:\n",
        "        shape = x.shape\n",
        "        print('Printing value of {} with shape {} :  {}'.format(name, shape, x))\n",
        "\n",
        "def extract_patches(x, offsets, size):\n",
        "    batch_size = x.shape[0]\n",
        "    sample_sizes = offsets.shape[1]\n",
        "    actual_images = x.squeeze(1)\n",
        "    all_patches = []\n",
        "    size_int = size.clone().int()\n",
        "    response = torch.ones((batch_size, sample_sizes, size_int[0].item(), size_int[1].item()), device = device, requires_grad=True)\n",
        "    for i in range(batch_size):\n",
        "        patches = []\n",
        "        for sample in range(sample_sizes):\n",
        "            left, top = offsets[i][sample][0].item() , offsets[i][sample][1].item()\n",
        "            left = max(0, left)\n",
        "            top = max(top, 0)\n",
        "            top = min(top, 1450)\n",
        "            left = min(left, 1450)\n",
        "            right, bottom = left + size[0].item() , top + size[1].item()\n",
        "            left, bottom, right, top = list(map(int, [left, bottom, right, top]))\n",
        "            patch = (actual_images[i][left: right,top: bottom])\n",
        "            response[i][sample] = patch\n",
        "    response = response.unsqueeze(-1)\n",
        "    return response\n",
        "\n",
        "def CategoricalCrossEntropyLoss(inputs, target):\n",
        "    _, labels = target.max(dim=1)\n",
        "    return F.cross_entropy(inputs, labels)\n",
        "def plot_grad_flow(named_parameters):\n",
        "    ave_grads = []\n",
        "    layers = []\n",
        "    for n, p in named_parameters:\n",
        "        if(p.requires_grad) and (\"bias\" not in n):\n",
        "            layers.append(n)\n",
        "            ave_grads.append(p.grad.abs().mean())\n",
        "    plt.plot(ave_grads, alpha=0.3, color=\"b\")\n",
        "    plt.hlines(0, 0, len(ave_grads)+1, linewidth=1, color=\"k\" )\n",
        "    plt.xticks(range(0,len(ave_grads), 1), layers, rotation=\"vertical\")\n",
        "    plt.xlim(xmin=0, xmax=len(ave_grads))\n",
        "    plt.xlabel(\"Layers\")\n",
        "    plt.ylabel(\"average gradient\")\n",
        "    plt.title(\"Gradient flow\")\n",
        "    plt.grid(True)\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "awesome = MegaMnistDataset('.', True)\n",
        "train_datalaoder = torch.utils.data.DataLoader(awesome, batch_size=20, shuffle=True, num_workers=5)\n",
        "net = AttentionSamplingModel(\n",
        "    output_size = 10,\n",
        "    patch_size = (50,50),\n",
        "    n_patches = 20,\n",
        "    replace = False\n",
        ")\n",
        "net = net.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum = 0.9)\n",
        "for epoch in range(250):\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_datalaoder, 0):\n",
        "        inputs, labels = data\n",
        "        inputs, labels = [inputs[0].to(device), inputs[1].to(device)], labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs, _loss = net(inputs)\n",
        "        loss = CategoricalCrossEntropyLoss(outputs, labels)\n",
        "        good_loss  = loss + _loss\n",
        "        good_loss.backward()\n",
        "        plot_grad_flow(net.named_parameters())\n",
        "        optimizer.step()\n",
        "        print('Epoch: ', epoch, ' Step: ', i, ' total ', len(train_datalaoder), 'Loss: ', loss)  \n",
        "\n",
        "print('Finished training')\n",
        "PATH = './amazing2_cifar_net.pth'\n",
        "torch.save(net.state_dict(), PATH)\n",
        "\n",
        "response = main()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:187: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:422: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:606: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:  0  Step:  0  total  250 Loss:  tensor(2.3031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  1  total  250 Loss:  tensor(2.3021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  2  total  250 Loss:  tensor(2.2993, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  3  total  250 Loss:  tensor(2.3026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  4  total  250 Loss:  tensor(2.3076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  5  total  250 Loss:  tensor(2.3042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  6  total  250 Loss:  tensor(2.3030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  7  total  250 Loss:  tensor(2.3067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  8  total  250 Loss:  tensor(2.3015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  9  total  250 Loss:  tensor(2.2981, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  10  total  250 Loss:  tensor(2.2995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  11  total  250 Loss:  tensor(2.3033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  12  total  250 Loss:  tensor(2.3015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  13  total  250 Loss:  tensor(2.3028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  14  total  250 Loss:  tensor(2.2994, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  15  total  250 Loss:  tensor(2.3036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  16  total  250 Loss:  tensor(2.3073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  17  total  250 Loss:  tensor(2.3115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  18  total  250 Loss:  tensor(2.3017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  19  total  250 Loss:  tensor(2.3033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  20  total  250 Loss:  tensor(2.3008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  21  total  250 Loss:  tensor(2.3054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  22  total  250 Loss:  tensor(2.3031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  23  total  250 Loss:  tensor(2.3032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  24  total  250 Loss:  tensor(2.2999, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  25  total  250 Loss:  tensor(2.3027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  26  total  250 Loss:  tensor(2.3009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  27  total  250 Loss:  tensor(2.3037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  28  total  250 Loss:  tensor(2.3041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  29  total  250 Loss:  tensor(2.3024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  30  total  250 Loss:  tensor(2.3036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  31  total  250 Loss:  tensor(2.3021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  32  total  250 Loss:  tensor(2.2970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  33  total  250 Loss:  tensor(2.3022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  34  total  250 Loss:  tensor(2.2979, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  35  total  250 Loss:  tensor(2.3071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  36  total  250 Loss:  tensor(2.3051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  37  total  250 Loss:  tensor(2.3048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  38  total  250 Loss:  tensor(2.3031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  39  total  250 Loss:  tensor(2.2984, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  40  total  250 Loss:  tensor(2.2998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  41  total  250 Loss:  tensor(2.3005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  42  total  250 Loss:  tensor(2.3015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  43  total  250 Loss:  tensor(2.2998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  44  total  250 Loss:  tensor(2.3007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  45  total  250 Loss:  tensor(2.3035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  46  total  250 Loss:  tensor(2.3010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  47  total  250 Loss:  tensor(2.2997, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  48  total  250 Loss:  tensor(2.3027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  49  total  250 Loss:  tensor(2.2985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  50  total  250 Loss:  tensor(2.3022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  51  total  250 Loss:  tensor(2.3039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  52  total  250 Loss:  tensor(2.3018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  53  total  250 Loss:  tensor(2.3061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  54  total  250 Loss:  tensor(2.3070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  55  total  250 Loss:  tensor(2.3033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  56  total  250 Loss:  tensor(2.3042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  57  total  250 Loss:  tensor(2.3061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  58  total  250 Loss:  tensor(2.3028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  59  total  250 Loss:  tensor(2.3043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  60  total  250 Loss:  tensor(2.3024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  61  total  250 Loss:  tensor(2.3008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  62  total  250 Loss:  tensor(2.3041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  63  total  250 Loss:  tensor(2.3017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  64  total  250 Loss:  tensor(2.3073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  65  total  250 Loss:  tensor(2.3039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  66  total  250 Loss:  tensor(2.2989, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  67  total  250 Loss:  tensor(2.3046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  68  total  250 Loss:  tensor(2.3067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  69  total  250 Loss:  tensor(2.3068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  70  total  250 Loss:  tensor(2.3003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  71  total  250 Loss:  tensor(2.3024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  72  total  250 Loss:  tensor(2.3024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  73  total  250 Loss:  tensor(2.3019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  74  total  250 Loss:  tensor(2.3035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  75  total  250 Loss:  tensor(2.3086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  76  total  250 Loss:  tensor(2.3018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  77  total  250 Loss:  tensor(2.3038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  78  total  250 Loss:  tensor(2.3009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  79  total  250 Loss:  tensor(2.3064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  80  total  250 Loss:  tensor(2.3015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  81  total  250 Loss:  tensor(2.3068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  82  total  250 Loss:  tensor(2.3046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  83  total  250 Loss:  tensor(2.3037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  84  total  250 Loss:  tensor(2.3033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  85  total  250 Loss:  tensor(2.3026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  86  total  250 Loss:  tensor(2.3040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  87  total  250 Loss:  tensor(2.3017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  88  total  250 Loss:  tensor(2.3050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  89  total  250 Loss:  tensor(2.3003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  90  total  250 Loss:  tensor(2.2983, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  91  total  250 Loss:  tensor(2.3040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  92  total  250 Loss:  tensor(2.3035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  93  total  250 Loss:  tensor(2.3058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  94  total  250 Loss:  tensor(2.3046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  95  total  250 Loss:  tensor(2.3008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  96  total  250 Loss:  tensor(2.2968, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  97  total  250 Loss:  tensor(2.3028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  98  total  250 Loss:  tensor(2.3023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  99  total  250 Loss:  tensor(2.3023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  100  total  250 Loss:  tensor(2.3008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  101  total  250 Loss:  tensor(2.3021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  102  total  250 Loss:  tensor(2.3021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  103  total  250 Loss:  tensor(2.3055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  104  total  250 Loss:  tensor(2.3033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  105  total  250 Loss:  tensor(2.3040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  106  total  250 Loss:  tensor(2.3036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  107  total  250 Loss:  tensor(2.2954, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  108  total  250 Loss:  tensor(2.3044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  109  total  250 Loss:  tensor(2.3047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  110  total  250 Loss:  tensor(2.3023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  111  total  250 Loss:  tensor(2.2978, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  112  total  250 Loss:  tensor(2.3006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  113  total  250 Loss:  tensor(2.2995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  114  total  250 Loss:  tensor(2.3092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  115  total  250 Loss:  tensor(2.3029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  116  total  250 Loss:  tensor(2.2989, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  117  total  250 Loss:  tensor(2.3000, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  118  total  250 Loss:  tensor(2.3026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  119  total  250 Loss:  tensor(2.2992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  120  total  250 Loss:  tensor(2.3011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  121  total  250 Loss:  tensor(2.3053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  122  total  250 Loss:  tensor(2.3019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  123  total  250 Loss:  tensor(2.3040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  124  total  250 Loss:  tensor(2.3018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  125  total  250 Loss:  tensor(2.2998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  126  total  250 Loss:  tensor(2.3050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  127  total  250 Loss:  tensor(2.2988, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  128  total  250 Loss:  tensor(2.3062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  129  total  250 Loss:  tensor(2.3060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  130  total  250 Loss:  tensor(2.3025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  131  total  250 Loss:  tensor(2.3001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  132  total  250 Loss:  tensor(2.3000, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  133  total  250 Loss:  tensor(2.3059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  134  total  250 Loss:  tensor(2.3011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  135  total  250 Loss:  tensor(2.3074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  136  total  250 Loss:  tensor(2.3009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  137  total  250 Loss:  tensor(2.3051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  138  total  250 Loss:  tensor(2.3022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  139  total  250 Loss:  tensor(2.3027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  140  total  250 Loss:  tensor(2.3052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  141  total  250 Loss:  tensor(2.3039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  142  total  250 Loss:  tensor(2.3004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  143  total  250 Loss:  tensor(2.3024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  144  total  250 Loss:  tensor(2.3018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  145  total  250 Loss:  tensor(2.3030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  146  total  250 Loss:  tensor(2.3044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  147  total  250 Loss:  tensor(2.3046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  148  total  250 Loss:  tensor(2.3031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  149  total  250 Loss:  tensor(2.3071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  150  total  250 Loss:  tensor(2.3020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  151  total  250 Loss:  tensor(2.2991, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  152  total  250 Loss:  tensor(2.3038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  153  total  250 Loss:  tensor(2.2986, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  154  total  250 Loss:  tensor(2.3042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  155  total  250 Loss:  tensor(2.3047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  156  total  250 Loss:  tensor(2.3027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  157  total  250 Loss:  tensor(2.3003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  158  total  250 Loss:  tensor(2.3017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  159  total  250 Loss:  tensor(2.3027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  160  total  250 Loss:  tensor(2.3020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  161  total  250 Loss:  tensor(2.3019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  162  total  250 Loss:  tensor(2.2999, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  163  total  250 Loss:  tensor(2.3020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  164  total  250 Loss:  tensor(2.3037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  165  total  250 Loss:  tensor(2.3024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  166  total  250 Loss:  tensor(2.3038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  167  total  250 Loss:  tensor(2.3029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  168  total  250 Loss:  tensor(2.3041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  169  total  250 Loss:  tensor(2.3039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  170  total  250 Loss:  tensor(2.3016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  171  total  250 Loss:  tensor(2.2988, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  172  total  250 Loss:  tensor(2.2998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  173  total  250 Loss:  tensor(2.3018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  174  total  250 Loss:  tensor(2.3018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  175  total  250 Loss:  tensor(2.3042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  176  total  250 Loss:  tensor(2.3040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  177  total  250 Loss:  tensor(2.3088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  178  total  250 Loss:  tensor(2.3023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  179  total  250 Loss:  tensor(2.3052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  180  total  250 Loss:  tensor(2.3040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  181  total  250 Loss:  tensor(2.2999, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  182  total  250 Loss:  tensor(2.3037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  183  total  250 Loss:  tensor(2.3044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  184  total  250 Loss:  tensor(2.3054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  185  total  250 Loss:  tensor(2.3021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  186  total  250 Loss:  tensor(2.3044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  187  total  250 Loss:  tensor(2.3021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  188  total  250 Loss:  tensor(2.3034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  189  total  250 Loss:  tensor(2.3020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  190  total  250 Loss:  tensor(2.3083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  191  total  250 Loss:  tensor(2.2997, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  192  total  250 Loss:  tensor(2.3002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  193  total  250 Loss:  tensor(2.3042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  194  total  250 Loss:  tensor(2.3012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  195  total  250 Loss:  tensor(2.3031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  196  total  250 Loss:  tensor(2.3071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  197  total  250 Loss:  tensor(2.3046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  198  total  250 Loss:  tensor(2.3064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  199  total  250 Loss:  tensor(2.3063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  200  total  250 Loss:  tensor(2.3031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  201  total  250 Loss:  tensor(2.3009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  202  total  250 Loss:  tensor(2.2943, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  203  total  250 Loss:  tensor(2.3043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  204  total  250 Loss:  tensor(2.3008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  205  total  250 Loss:  tensor(2.3031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  206  total  250 Loss:  tensor(2.3014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  207  total  250 Loss:  tensor(2.3062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  208  total  250 Loss:  tensor(2.3033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  209  total  250 Loss:  tensor(2.3011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  210  total  250 Loss:  tensor(2.3042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  211  total  250 Loss:  tensor(2.3046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  212  total  250 Loss:  tensor(2.3006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  213  total  250 Loss:  tensor(2.3008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  214  total  250 Loss:  tensor(2.3031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  215  total  250 Loss:  tensor(2.2993, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  216  total  250 Loss:  tensor(2.2993, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  217  total  250 Loss:  tensor(2.3034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  218  total  250 Loss:  tensor(2.3060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  219  total  250 Loss:  tensor(2.3038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  220  total  250 Loss:  tensor(2.3058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  221  total  250 Loss:  tensor(2.3037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  222  total  250 Loss:  tensor(2.2988, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  223  total  250 Loss:  tensor(2.3026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  224  total  250 Loss:  tensor(2.2988, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  225  total  250 Loss:  tensor(2.3034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  226  total  250 Loss:  tensor(2.2969, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  227  total  250 Loss:  tensor(2.3031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  228  total  250 Loss:  tensor(2.3100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  229  total  250 Loss:  tensor(2.3020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  230  total  250 Loss:  tensor(2.3022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  231  total  250 Loss:  tensor(2.3027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  232  total  250 Loss:  tensor(2.3025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  233  total  250 Loss:  tensor(2.3050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  234  total  250 Loss:  tensor(2.3009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  235  total  250 Loss:  tensor(2.3055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  236  total  250 Loss:  tensor(2.3021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  237  total  250 Loss:  tensor(2.3029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  238  total  250 Loss:  tensor(2.3004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  239  total  250 Loss:  tensor(2.3014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  240  total  250 Loss:  tensor(2.3037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  241  total  250 Loss:  tensor(2.3063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  242  total  250 Loss:  tensor(2.3000, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  243  total  250 Loss:  tensor(2.3069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  244  total  250 Loss:  tensor(2.3030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  245  total  250 Loss:  tensor(2.3028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  246  total  250 Loss:  tensor(2.3006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  247  total  250 Loss:  tensor(2.2971, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  248  total  250 Loss:  tensor(2.3048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  0  Step:  249  total  250 Loss:  tensor(2.3010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  0  total  250 Loss:  tensor(2.3006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  1  total  250 Loss:  tensor(2.2953, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  2  total  250 Loss:  tensor(2.3087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  3  total  250 Loss:  tensor(2.3003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  4  total  250 Loss:  tensor(2.3023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  5  total  250 Loss:  tensor(2.3039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  6  total  250 Loss:  tensor(2.3008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  7  total  250 Loss:  tensor(2.3020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  8  total  250 Loss:  tensor(2.3091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  9  total  250 Loss:  tensor(2.3018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  10  total  250 Loss:  tensor(2.3057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  11  total  250 Loss:  tensor(2.3020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  12  total  250 Loss:  tensor(2.3018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  13  total  250 Loss:  tensor(2.3045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  14  total  250 Loss:  tensor(2.2964, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  15  total  250 Loss:  tensor(2.3055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  16  total  250 Loss:  tensor(2.3010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  17  total  250 Loss:  tensor(2.3019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  18  total  250 Loss:  tensor(2.3039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  19  total  250 Loss:  tensor(2.3006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  20  total  250 Loss:  tensor(2.3004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  21  total  250 Loss:  tensor(2.2992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  22  total  250 Loss:  tensor(2.3031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  23  total  250 Loss:  tensor(2.2993, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  24  total  250 Loss:  tensor(2.3051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  25  total  250 Loss:  tensor(2.3032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  26  total  250 Loss:  tensor(2.2983, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  27  total  250 Loss:  tensor(2.3058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  28  total  250 Loss:  tensor(2.3074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  29  total  250 Loss:  tensor(2.3063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  30  total  250 Loss:  tensor(2.3013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  31  total  250 Loss:  tensor(2.3033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  32  total  250 Loss:  tensor(2.3045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  33  total  250 Loss:  tensor(2.3052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  34  total  250 Loss:  tensor(2.2973, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  35  total  250 Loss:  tensor(2.3050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  36  total  250 Loss:  tensor(2.3071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  37  total  250 Loss:  tensor(2.3010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  38  total  250 Loss:  tensor(2.3010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  39  total  250 Loss:  tensor(2.3002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  40  total  250 Loss:  tensor(2.3055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  41  total  250 Loss:  tensor(2.3005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  42  total  250 Loss:  tensor(2.3031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  43  total  250 Loss:  tensor(2.3018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  44  total  250 Loss:  tensor(2.3015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  45  total  250 Loss:  tensor(2.3034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  46  total  250 Loss:  tensor(2.3018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  47  total  250 Loss:  tensor(2.3019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  48  total  250 Loss:  tensor(2.3033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  49  total  250 Loss:  tensor(2.3046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  50  total  250 Loss:  tensor(2.3051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  51  total  250 Loss:  tensor(2.3021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  52  total  250 Loss:  tensor(2.3016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  53  total  250 Loss:  tensor(2.2993, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  54  total  250 Loss:  tensor(2.3075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  55  total  250 Loss:  tensor(2.3050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  56  total  250 Loss:  tensor(2.3089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  57  total  250 Loss:  tensor(2.3015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  58  total  250 Loss:  tensor(2.3000, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  59  total  250 Loss:  tensor(2.3034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  60  total  250 Loss:  tensor(2.3067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  61  total  250 Loss:  tensor(2.3011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  62  total  250 Loss:  tensor(2.3025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  63  total  250 Loss:  tensor(2.3025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  64  total  250 Loss:  tensor(2.2986, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  65  total  250 Loss:  tensor(2.3009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  66  total  250 Loss:  tensor(2.3068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  67  total  250 Loss:  tensor(2.3026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  68  total  250 Loss:  tensor(2.3011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  69  total  250 Loss:  tensor(2.3047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  70  total  250 Loss:  tensor(2.3028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  71  total  250 Loss:  tensor(2.3033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  72  total  250 Loss:  tensor(2.3063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  73  total  250 Loss:  tensor(2.3019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  74  total  250 Loss:  tensor(2.3016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  75  total  250 Loss:  tensor(2.3001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  76  total  250 Loss:  tensor(2.2998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  77  total  250 Loss:  tensor(2.3027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  78  total  250 Loss:  tensor(2.3028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  79  total  250 Loss:  tensor(2.3039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  80  total  250 Loss:  tensor(2.3037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  81  total  250 Loss:  tensor(2.3017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  82  total  250 Loss:  tensor(2.3024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  83  total  250 Loss:  tensor(2.3024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  84  total  250 Loss:  tensor(2.3004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  85  total  250 Loss:  tensor(2.3003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  86  total  250 Loss:  tensor(2.3034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  87  total  250 Loss:  tensor(2.3037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  88  total  250 Loss:  tensor(2.3022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  89  total  250 Loss:  tensor(2.3003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  90  total  250 Loss:  tensor(2.3017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  91  total  250 Loss:  tensor(2.3075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  92  total  250 Loss:  tensor(2.3007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  93  total  250 Loss:  tensor(2.3028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  94  total  250 Loss:  tensor(2.3013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  95  total  250 Loss:  tensor(2.3036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  96  total  250 Loss:  tensor(2.2998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  97  total  250 Loss:  tensor(2.3036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  98  total  250 Loss:  tensor(2.2987, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  99  total  250 Loss:  tensor(2.3049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  100  total  250 Loss:  tensor(2.3026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  101  total  250 Loss:  tensor(2.2995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  102  total  250 Loss:  tensor(2.3026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  103  total  250 Loss:  tensor(2.3009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  104  total  250 Loss:  tensor(2.3051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  105  total  250 Loss:  tensor(2.3044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  106  total  250 Loss:  tensor(2.3069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  107  total  250 Loss:  tensor(2.3030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  108  total  250 Loss:  tensor(2.3033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  109  total  250 Loss:  tensor(2.3014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  110  total  250 Loss:  tensor(2.2998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  111  total  250 Loss:  tensor(2.3069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  112  total  250 Loss:  tensor(2.3041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  113  total  250 Loss:  tensor(2.3041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  114  total  250 Loss:  tensor(2.3007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  115  total  250 Loss:  tensor(2.3068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  116  total  250 Loss:  tensor(2.3038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  117  total  250 Loss:  tensor(2.3001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  118  total  250 Loss:  tensor(2.3029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  119  total  250 Loss:  tensor(2.3033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  120  total  250 Loss:  tensor(2.3020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  121  total  250 Loss:  tensor(2.3067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  122  total  250 Loss:  tensor(2.3030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  123  total  250 Loss:  tensor(2.3000, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  124  total  250 Loss:  tensor(2.3029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  125  total  250 Loss:  tensor(2.2985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  126  total  250 Loss:  tensor(2.3058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  127  total  250 Loss:  tensor(2.3005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  128  total  250 Loss:  tensor(2.3064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  129  total  250 Loss:  tensor(2.3012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  130  total  250 Loss:  tensor(2.3079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  131  total  250 Loss:  tensor(2.3023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  132  total  250 Loss:  tensor(2.3049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  133  total  250 Loss:  tensor(2.3085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  134  total  250 Loss:  tensor(2.3000, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  135  total  250 Loss:  tensor(2.2992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  136  total  250 Loss:  tensor(2.3001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  137  total  250 Loss:  tensor(2.3022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  138  total  250 Loss:  tensor(2.2988, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  139  total  250 Loss:  tensor(2.3033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  140  total  250 Loss:  tensor(2.3005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  141  total  250 Loss:  tensor(2.3015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  142  total  250 Loss:  tensor(2.2981, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  143  total  250 Loss:  tensor(2.3036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  144  total  250 Loss:  tensor(2.3028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  145  total  250 Loss:  tensor(2.3043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  146  total  250 Loss:  tensor(2.3045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  147  total  250 Loss:  tensor(2.3104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  148  total  250 Loss:  tensor(2.2998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  149  total  250 Loss:  tensor(2.3023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  150  total  250 Loss:  tensor(2.2986, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  151  total  250 Loss:  tensor(2.2977, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  152  total  250 Loss:  tensor(2.3056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  153  total  250 Loss:  tensor(2.3001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  154  total  250 Loss:  tensor(2.3009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  155  total  250 Loss:  tensor(2.3017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  156  total  250 Loss:  tensor(2.3001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  157  total  250 Loss:  tensor(2.3007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  158  total  250 Loss:  tensor(2.3002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  159  total  250 Loss:  tensor(2.3043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  160  total  250 Loss:  tensor(2.3060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  161  total  250 Loss:  tensor(2.3046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  162  total  250 Loss:  tensor(2.2997, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  163  total  250 Loss:  tensor(2.3026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  164  total  250 Loss:  tensor(2.3069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  165  total  250 Loss:  tensor(2.3019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  166  total  250 Loss:  tensor(2.2989, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  167  total  250 Loss:  tensor(2.3046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  168  total  250 Loss:  tensor(2.2983, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  169  total  250 Loss:  tensor(2.2959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  170  total  250 Loss:  tensor(2.2985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  171  total  250 Loss:  tensor(2.3009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  172  total  250 Loss:  tensor(2.3076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  173  total  250 Loss:  tensor(2.3052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  174  total  250 Loss:  tensor(2.3031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  175  total  250 Loss:  tensor(2.3008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  176  total  250 Loss:  tensor(2.3087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  177  total  250 Loss:  tensor(2.3025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  178  total  250 Loss:  tensor(2.3083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  179  total  250 Loss:  tensor(2.3006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  180  total  250 Loss:  tensor(2.2982, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  181  total  250 Loss:  tensor(2.3031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  182  total  250 Loss:  tensor(2.3067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  183  total  250 Loss:  tensor(2.3037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  184  total  250 Loss:  tensor(2.3043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  185  total  250 Loss:  tensor(2.3007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  186  total  250 Loss:  tensor(2.3029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  187  total  250 Loss:  tensor(2.3001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  188  total  250 Loss:  tensor(2.3019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  189  total  250 Loss:  tensor(2.3008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  190  total  250 Loss:  tensor(2.3042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  191  total  250 Loss:  tensor(2.2997, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  192  total  250 Loss:  tensor(2.3007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  193  total  250 Loss:  tensor(2.3066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  194  total  250 Loss:  tensor(2.3039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  195  total  250 Loss:  tensor(2.3065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  196  total  250 Loss:  tensor(2.3062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  197  total  250 Loss:  tensor(2.3024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  198  total  250 Loss:  tensor(2.2992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  199  total  250 Loss:  tensor(2.3025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  200  total  250 Loss:  tensor(2.3005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  201  total  250 Loss:  tensor(2.3068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  202  total  250 Loss:  tensor(2.3026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  203  total  250 Loss:  tensor(2.3054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  204  total  250 Loss:  tensor(2.3027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  205  total  250 Loss:  tensor(2.3049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  206  total  250 Loss:  tensor(2.3057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  207  total  250 Loss:  tensor(2.3005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  208  total  250 Loss:  tensor(2.2995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  209  total  250 Loss:  tensor(2.3059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  210  total  250 Loss:  tensor(2.3012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  211  total  250 Loss:  tensor(2.3044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  212  total  250 Loss:  tensor(2.2986, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  213  total  250 Loss:  tensor(2.3017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  214  total  250 Loss:  tensor(2.3007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  215  total  250 Loss:  tensor(2.2997, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  216  total  250 Loss:  tensor(2.3068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  217  total  250 Loss:  tensor(2.3011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  218  total  250 Loss:  tensor(2.3014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  219  total  250 Loss:  tensor(2.3033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  220  total  250 Loss:  tensor(2.2994, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  221  total  250 Loss:  tensor(2.3025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  222  total  250 Loss:  tensor(2.2963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  223  total  250 Loss:  tensor(2.3037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  224  total  250 Loss:  tensor(2.3060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  225  total  250 Loss:  tensor(2.3042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  226  total  250 Loss:  tensor(2.3059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  227  total  250 Loss:  tensor(2.2993, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  228  total  250 Loss:  tensor(2.3012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  229  total  250 Loss:  tensor(2.3022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  230  total  250 Loss:  tensor(2.3039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  231  total  250 Loss:  tensor(2.3071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  232  total  250 Loss:  tensor(2.3016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  233  total  250 Loss:  tensor(2.2985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  234  total  250 Loss:  tensor(2.3032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  235  total  250 Loss:  tensor(2.3077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  236  total  250 Loss:  tensor(2.3000, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  237  total  250 Loss:  tensor(2.3051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  238  total  250 Loss:  tensor(2.3056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  239  total  250 Loss:  tensor(2.3054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  240  total  250 Loss:  tensor(2.3054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  241  total  250 Loss:  tensor(2.2990, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  242  total  250 Loss:  tensor(2.3037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  243  total  250 Loss:  tensor(2.2985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  244  total  250 Loss:  tensor(2.3031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  245  total  250 Loss:  tensor(2.3051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  246  total  250 Loss:  tensor(2.3025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  247  total  250 Loss:  tensor(2.3028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  248  total  250 Loss:  tensor(2.3042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  1  Step:  249  total  250 Loss:  tensor(2.3025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  0  total  250 Loss:  tensor(2.3014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  1  total  250 Loss:  tensor(2.2967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  2  total  250 Loss:  tensor(2.2990, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  3  total  250 Loss:  tensor(2.3001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  4  total  250 Loss:  tensor(2.2999, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  5  total  250 Loss:  tensor(2.3002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  6  total  250 Loss:  tensor(2.2977, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  7  total  250 Loss:  tensor(2.3042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  8  total  250 Loss:  tensor(2.3025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  9  total  250 Loss:  tensor(2.3029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  10  total  250 Loss:  tensor(2.3043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  11  total  250 Loss:  tensor(2.3013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  12  total  250 Loss:  tensor(2.3026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  13  total  250 Loss:  tensor(2.3043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  14  total  250 Loss:  tensor(2.3026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  15  total  250 Loss:  tensor(2.3016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  16  total  250 Loss:  tensor(2.3015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  17  total  250 Loss:  tensor(2.3011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  18  total  250 Loss:  tensor(2.3011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  19  total  250 Loss:  tensor(2.3031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  20  total  250 Loss:  tensor(2.3015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  21  total  250 Loss:  tensor(2.3037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  22  total  250 Loss:  tensor(2.3026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  23  total  250 Loss:  tensor(2.3029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  24  total  250 Loss:  tensor(2.3037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  25  total  250 Loss:  tensor(2.3014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  26  total  250 Loss:  tensor(2.3025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  27  total  250 Loss:  tensor(2.2997, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  28  total  250 Loss:  tensor(2.3038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  29  total  250 Loss:  tensor(2.3022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  30  total  250 Loss:  tensor(2.3005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  31  total  250 Loss:  tensor(2.3052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  32  total  250 Loss:  tensor(2.3014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  33  total  250 Loss:  tensor(2.3038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  34  total  250 Loss:  tensor(2.3030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  35  total  250 Loss:  tensor(2.3026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  36  total  250 Loss:  tensor(2.3070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  37  total  250 Loss:  tensor(2.3000, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  38  total  250 Loss:  tensor(2.3028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  39  total  250 Loss:  tensor(2.3017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  40  total  250 Loss:  tensor(2.3040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  41  total  250 Loss:  tensor(2.3053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  42  total  250 Loss:  tensor(2.2994, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  43  total  250 Loss:  tensor(2.2971, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  44  total  250 Loss:  tensor(2.3054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  45  total  250 Loss:  tensor(2.3012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  46  total  250 Loss:  tensor(2.3029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  47  total  250 Loss:  tensor(2.3054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  48  total  250 Loss:  tensor(2.3042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  49  total  250 Loss:  tensor(2.2992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  50  total  250 Loss:  tensor(2.3037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  51  total  250 Loss:  tensor(2.3015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  52  total  250 Loss:  tensor(2.3043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  53  total  250 Loss:  tensor(2.3059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  54  total  250 Loss:  tensor(2.3040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  55  total  250 Loss:  tensor(2.3038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  56  total  250 Loss:  tensor(2.3028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  57  total  250 Loss:  tensor(2.3057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  58  total  250 Loss:  tensor(2.3028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  59  total  250 Loss:  tensor(2.3025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  60  total  250 Loss:  tensor(2.2998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  61  total  250 Loss:  tensor(2.3040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  62  total  250 Loss:  tensor(2.3051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  63  total  250 Loss:  tensor(2.3041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  64  total  250 Loss:  tensor(2.3031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  65  total  250 Loss:  tensor(2.3027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  66  total  250 Loss:  tensor(2.3018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  67  total  250 Loss:  tensor(2.3026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  68  total  250 Loss:  tensor(2.3024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  69  total  250 Loss:  tensor(2.3047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  70  total  250 Loss:  tensor(2.3027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  71  total  250 Loss:  tensor(2.3002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  72  total  250 Loss:  tensor(2.3014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  73  total  250 Loss:  tensor(2.3028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  74  total  250 Loss:  tensor(2.3027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  75  total  250 Loss:  tensor(2.3053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  76  total  250 Loss:  tensor(2.3056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  77  total  250 Loss:  tensor(2.2998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  78  total  250 Loss:  tensor(2.3007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  79  total  250 Loss:  tensor(2.3035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  80  total  250 Loss:  tensor(2.3021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  81  total  250 Loss:  tensor(2.2984, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  82  total  250 Loss:  tensor(2.3018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  83  total  250 Loss:  tensor(2.3085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  84  total  250 Loss:  tensor(2.3042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  85  total  250 Loss:  tensor(2.3024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  86  total  250 Loss:  tensor(2.3058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  87  total  250 Loss:  tensor(2.3047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  88  total  250 Loss:  tensor(2.3006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  89  total  250 Loss:  tensor(2.3021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  90  total  250 Loss:  tensor(2.3012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  91  total  250 Loss:  tensor(2.3017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  92  total  250 Loss:  tensor(2.3025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  93  total  250 Loss:  tensor(2.2982, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  94  total  250 Loss:  tensor(2.3015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  95  total  250 Loss:  tensor(2.3048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  96  total  250 Loss:  tensor(2.2991, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  97  total  250 Loss:  tensor(2.3036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  98  total  250 Loss:  tensor(2.2994, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  99  total  250 Loss:  tensor(2.3062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  100  total  250 Loss:  tensor(2.2996, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  101  total  250 Loss:  tensor(2.3037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  102  total  250 Loss:  tensor(2.3031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  103  total  250 Loss:  tensor(2.3032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  104  total  250 Loss:  tensor(2.3034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  105  total  250 Loss:  tensor(2.3023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  106  total  250 Loss:  tensor(2.3043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  107  total  250 Loss:  tensor(2.3018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  108  total  250 Loss:  tensor(2.3001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  109  total  250 Loss:  tensor(2.3058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  110  total  250 Loss:  tensor(2.3023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  111  total  250 Loss:  tensor(2.3029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  112  total  250 Loss:  tensor(2.3056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  113  total  250 Loss:  tensor(2.3026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  114  total  250 Loss:  tensor(2.3035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  115  total  250 Loss:  tensor(2.3045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  116  total  250 Loss:  tensor(2.3036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  117  total  250 Loss:  tensor(2.2997, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  118  total  250 Loss:  tensor(2.2994, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  119  total  250 Loss:  tensor(2.3028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  120  total  250 Loss:  tensor(2.3078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  121  total  250 Loss:  tensor(2.3013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  122  total  250 Loss:  tensor(2.2966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  123  total  250 Loss:  tensor(2.2994, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  124  total  250 Loss:  tensor(2.2987, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  125  total  250 Loss:  tensor(2.3072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  126  total  250 Loss:  tensor(2.3020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  127  total  250 Loss:  tensor(2.3006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  128  total  250 Loss:  tensor(2.3046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  129  total  250 Loss:  tensor(2.2990, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  130  total  250 Loss:  tensor(2.3055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  131  total  250 Loss:  tensor(2.3006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  132  total  250 Loss:  tensor(2.3069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  133  total  250 Loss:  tensor(2.3028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  134  total  250 Loss:  tensor(2.3016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  135  total  250 Loss:  tensor(2.3019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  136  total  250 Loss:  tensor(2.3048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  137  total  250 Loss:  tensor(2.3048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  138  total  250 Loss:  tensor(2.3012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  139  total  250 Loss:  tensor(2.3046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  140  total  250 Loss:  tensor(2.3069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  141  total  250 Loss:  tensor(2.3060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  142  total  250 Loss:  tensor(2.2999, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  143  total  250 Loss:  tensor(2.3033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  144  total  250 Loss:  tensor(2.3001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  145  total  250 Loss:  tensor(2.3037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  146  total  250 Loss:  tensor(2.3043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  147  total  250 Loss:  tensor(2.3046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  148  total  250 Loss:  tensor(2.3002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  149  total  250 Loss:  tensor(2.3051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  150  total  250 Loss:  tensor(2.3008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  151  total  250 Loss:  tensor(2.3049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  152  total  250 Loss:  tensor(2.3067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  153  total  250 Loss:  tensor(2.3000, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  154  total  250 Loss:  tensor(2.3021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  155  total  250 Loss:  tensor(2.3018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  156  total  250 Loss:  tensor(2.3018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  157  total  250 Loss:  tensor(2.3060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  158  total  250 Loss:  tensor(2.3068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  159  total  250 Loss:  tensor(2.3037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  160  total  250 Loss:  tensor(2.3067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  161  total  250 Loss:  tensor(2.3002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  162  total  250 Loss:  tensor(2.3047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  163  total  250 Loss:  tensor(2.3008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  164  total  250 Loss:  tensor(2.3003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  165  total  250 Loss:  tensor(2.3031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  166  total  250 Loss:  tensor(2.3014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  167  total  250 Loss:  tensor(2.3019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  168  total  250 Loss:  tensor(2.3014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  169  total  250 Loss:  tensor(2.3087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  170  total  250 Loss:  tensor(2.3023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  171  total  250 Loss:  tensor(2.3098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  172  total  250 Loss:  tensor(2.3028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  173  total  250 Loss:  tensor(2.3036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  174  total  250 Loss:  tensor(2.3008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  175  total  250 Loss:  tensor(2.3059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  176  total  250 Loss:  tensor(2.3082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  177  total  250 Loss:  tensor(2.3044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  178  total  250 Loss:  tensor(2.3029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  179  total  250 Loss:  tensor(2.3049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  180  total  250 Loss:  tensor(2.3052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  181  total  250 Loss:  tensor(2.3018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  182  total  250 Loss:  tensor(2.3007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  183  total  250 Loss:  tensor(2.3005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  184  total  250 Loss:  tensor(2.3065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  185  total  250 Loss:  tensor(2.3040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  186  total  250 Loss:  tensor(2.3050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  187  total  250 Loss:  tensor(2.3037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  188  total  250 Loss:  tensor(2.3067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  189  total  250 Loss:  tensor(2.3009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  190  total  250 Loss:  tensor(2.2978, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  191  total  250 Loss:  tensor(2.3023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  192  total  250 Loss:  tensor(2.2975, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  193  total  250 Loss:  tensor(2.3041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  194  total  250 Loss:  tensor(2.3016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  195  total  250 Loss:  tensor(2.3037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  196  total  250 Loss:  tensor(2.3006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  197  total  250 Loss:  tensor(2.3013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  198  total  250 Loss:  tensor(2.2993, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  199  total  250 Loss:  tensor(2.3037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  200  total  250 Loss:  tensor(2.3004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  201  total  250 Loss:  tensor(2.3032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  202  total  250 Loss:  tensor(2.3016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  203  total  250 Loss:  tensor(2.3045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  204  total  250 Loss:  tensor(2.2997, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  205  total  250 Loss:  tensor(2.3029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  206  total  250 Loss:  tensor(2.3014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  207  total  250 Loss:  tensor(2.3012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  208  total  250 Loss:  tensor(2.3066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  209  total  250 Loss:  tensor(2.3032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  210  total  250 Loss:  tensor(2.3001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  211  total  250 Loss:  tensor(2.3030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  212  total  250 Loss:  tensor(2.3018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  213  total  250 Loss:  tensor(2.3010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  214  total  250 Loss:  tensor(2.3070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  215  total  250 Loss:  tensor(2.3008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  216  total  250 Loss:  tensor(2.2997, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  217  total  250 Loss:  tensor(2.3058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  218  total  250 Loss:  tensor(2.3055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  219  total  250 Loss:  tensor(2.2992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  220  total  250 Loss:  tensor(2.2969, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  221  total  250 Loss:  tensor(2.3037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  222  total  250 Loss:  tensor(2.2991, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  223  total  250 Loss:  tensor(2.3019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  224  total  250 Loss:  tensor(2.3005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  225  total  250 Loss:  tensor(2.3023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  226  total  250 Loss:  tensor(2.3033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  227  total  250 Loss:  tensor(2.3031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  228  total  250 Loss:  tensor(2.3025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  229  total  250 Loss:  tensor(2.3042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  230  total  250 Loss:  tensor(2.3007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  231  total  250 Loss:  tensor(2.3021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  232  total  250 Loss:  tensor(2.3038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  233  total  250 Loss:  tensor(2.2995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  234  total  250 Loss:  tensor(2.3037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  235  total  250 Loss:  tensor(2.3024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  236  total  250 Loss:  tensor(2.3050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  237  total  250 Loss:  tensor(2.3008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  238  total  250 Loss:  tensor(2.3091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  239  total  250 Loss:  tensor(2.3043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  240  total  250 Loss:  tensor(2.2983, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  241  total  250 Loss:  tensor(2.3025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  242  total  250 Loss:  tensor(2.3021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  243  total  250 Loss:  tensor(2.3062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  244  total  250 Loss:  tensor(2.3056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  245  total  250 Loss:  tensor(2.3060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  246  total  250 Loss:  tensor(2.3033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  247  total  250 Loss:  tensor(2.3036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  248  total  250 Loss:  tensor(2.3027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  2  Step:  249  total  250 Loss:  tensor(2.3010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  0  total  250 Loss:  tensor(2.3007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  1  total  250 Loss:  tensor(2.3017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  2  total  250 Loss:  tensor(2.3038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  3  total  250 Loss:  tensor(2.3039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  4  total  250 Loss:  tensor(2.3042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  5  total  250 Loss:  tensor(2.2989, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  6  total  250 Loss:  tensor(2.3022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  7  total  250 Loss:  tensor(2.3025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  8  total  250 Loss:  tensor(2.3075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  9  total  250 Loss:  tensor(2.3005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  10  total  250 Loss:  tensor(2.3063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  11  total  250 Loss:  tensor(2.3036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  12  total  250 Loss:  tensor(2.3064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  13  total  250 Loss:  tensor(2.3049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  14  total  250 Loss:  tensor(2.2985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  15  total  250 Loss:  tensor(2.3061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  16  total  250 Loss:  tensor(2.3007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  17  total  250 Loss:  tensor(2.3028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  18  total  250 Loss:  tensor(2.3043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  19  total  250 Loss:  tensor(2.3005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  20  total  250 Loss:  tensor(2.3043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  21  total  250 Loss:  tensor(2.3027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  22  total  250 Loss:  tensor(2.2999, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  23  total  250 Loss:  tensor(2.2980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  24  total  250 Loss:  tensor(2.3018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  25  total  250 Loss:  tensor(2.3020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  26  total  250 Loss:  tensor(2.3010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  27  total  250 Loss:  tensor(2.3024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  28  total  250 Loss:  tensor(2.3016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  29  total  250 Loss:  tensor(2.3078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  30  total  250 Loss:  tensor(2.3028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  31  total  250 Loss:  tensor(2.2980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  32  total  250 Loss:  tensor(2.2983, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  33  total  250 Loss:  tensor(2.3022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  34  total  250 Loss:  tensor(2.3033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  35  total  250 Loss:  tensor(2.3042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  36  total  250 Loss:  tensor(2.3025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  37  total  250 Loss:  tensor(2.3010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  38  total  250 Loss:  tensor(2.2976, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  39  total  250 Loss:  tensor(2.3044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  40  total  250 Loss:  tensor(2.3010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  41  total  250 Loss:  tensor(2.3015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  42  total  250 Loss:  tensor(2.3067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  43  total  250 Loss:  tensor(2.3025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  44  total  250 Loss:  tensor(2.3005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  45  total  250 Loss:  tensor(2.2956, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  46  total  250 Loss:  tensor(2.3070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  47  total  250 Loss:  tensor(2.3023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  48  total  250 Loss:  tensor(2.2997, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  49  total  250 Loss:  tensor(2.3066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  50  total  250 Loss:  tensor(2.3028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  51  total  250 Loss:  tensor(2.3040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  52  total  250 Loss:  tensor(2.3063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  53  total  250 Loss:  tensor(2.3033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  54  total  250 Loss:  tensor(2.2981, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  55  total  250 Loss:  tensor(2.2991, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  56  total  250 Loss:  tensor(2.2995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  57  total  250 Loss:  tensor(2.3057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  58  total  250 Loss:  tensor(2.3080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  59  total  250 Loss:  tensor(2.3025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  60  total  250 Loss:  tensor(2.3016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  61  total  250 Loss:  tensor(2.3047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  62  total  250 Loss:  tensor(2.3039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  63  total  250 Loss:  tensor(2.3018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  64  total  250 Loss:  tensor(2.3051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  65  total  250 Loss:  tensor(2.3038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  66  total  250 Loss:  tensor(2.3051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  67  total  250 Loss:  tensor(2.3052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  68  total  250 Loss:  tensor(2.3002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  69  total  250 Loss:  tensor(2.3016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  70  total  250 Loss:  tensor(2.3047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  71  total  250 Loss:  tensor(2.3060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  72  total  250 Loss:  tensor(2.3060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  73  total  250 Loss:  tensor(2.3051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  74  total  250 Loss:  tensor(2.3026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  75  total  250 Loss:  tensor(2.3013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  76  total  250 Loss:  tensor(2.3069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  77  total  250 Loss:  tensor(2.3064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  78  total  250 Loss:  tensor(2.3047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  79  total  250 Loss:  tensor(2.3030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  80  total  250 Loss:  tensor(2.3055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  81  total  250 Loss:  tensor(2.3017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  82  total  250 Loss:  tensor(2.2972, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  83  total  250 Loss:  tensor(2.3040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  84  total  250 Loss:  tensor(2.3009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  85  total  250 Loss:  tensor(2.3058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  86  total  250 Loss:  tensor(2.3036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  87  total  250 Loss:  tensor(2.3009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  88  total  250 Loss:  tensor(2.3011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  89  total  250 Loss:  tensor(2.3007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  90  total  250 Loss:  tensor(2.3054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  91  total  250 Loss:  tensor(2.2988, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  92  total  250 Loss:  tensor(2.3044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  93  total  250 Loss:  tensor(2.3083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  94  total  250 Loss:  tensor(2.3033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  95  total  250 Loss:  tensor(2.2992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  96  total  250 Loss:  tensor(2.3049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  97  total  250 Loss:  tensor(2.3078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  98  total  250 Loss:  tensor(2.2993, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  99  total  250 Loss:  tensor(2.2986, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  100  total  250 Loss:  tensor(2.3060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  101  total  250 Loss:  tensor(2.3042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  102  total  250 Loss:  tensor(2.3029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  103  total  250 Loss:  tensor(2.3021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  104  total  250 Loss:  tensor(2.3049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  105  total  250 Loss:  tensor(2.3028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  106  total  250 Loss:  tensor(2.3035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  107  total  250 Loss:  tensor(2.3075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  108  total  250 Loss:  tensor(2.3048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  109  total  250 Loss:  tensor(2.2991, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  110  total  250 Loss:  tensor(2.3003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  111  total  250 Loss:  tensor(2.3008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  112  total  250 Loss:  tensor(2.3057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  113  total  250 Loss:  tensor(2.3024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  114  total  250 Loss:  tensor(2.3010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  115  total  250 Loss:  tensor(2.3000, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  116  total  250 Loss:  tensor(2.3034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  117  total  250 Loss:  tensor(2.3020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  118  total  250 Loss:  tensor(2.3025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  119  total  250 Loss:  tensor(2.3020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  120  total  250 Loss:  tensor(2.3001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  121  total  250 Loss:  tensor(2.3009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  122  total  250 Loss:  tensor(2.2993, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  123  total  250 Loss:  tensor(2.3095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  124  total  250 Loss:  tensor(2.3009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  125  total  250 Loss:  tensor(2.3055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  126  total  250 Loss:  tensor(2.3011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  127  total  250 Loss:  tensor(2.2998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  128  total  250 Loss:  tensor(2.3042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  129  total  250 Loss:  tensor(2.3021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  130  total  250 Loss:  tensor(2.3029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  131  total  250 Loss:  tensor(2.2987, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  132  total  250 Loss:  tensor(2.3021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  133  total  250 Loss:  tensor(2.3003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  134  total  250 Loss:  tensor(2.3025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  135  total  250 Loss:  tensor(2.2998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  136  total  250 Loss:  tensor(2.3058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  137  total  250 Loss:  tensor(2.2972, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  138  total  250 Loss:  tensor(2.3047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  139  total  250 Loss:  tensor(2.3050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  140  total  250 Loss:  tensor(2.3046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  141  total  250 Loss:  tensor(2.3021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  142  total  250 Loss:  tensor(2.3054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  143  total  250 Loss:  tensor(2.3039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  144  total  250 Loss:  tensor(2.3015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  145  total  250 Loss:  tensor(2.3026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  146  total  250 Loss:  tensor(2.3045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  147  total  250 Loss:  tensor(2.3050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  148  total  250 Loss:  tensor(2.3074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  149  total  250 Loss:  tensor(2.3049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  150  total  250 Loss:  tensor(2.3020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  151  total  250 Loss:  tensor(2.3064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  152  total  250 Loss:  tensor(2.3057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  153  total  250 Loss:  tensor(2.3053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  154  total  250 Loss:  tensor(2.3028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  155  total  250 Loss:  tensor(2.3043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  156  total  250 Loss:  tensor(2.3054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  157  total  250 Loss:  tensor(2.3019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  158  total  250 Loss:  tensor(2.3050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  159  total  250 Loss:  tensor(2.2983, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  160  total  250 Loss:  tensor(2.3040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  161  total  250 Loss:  tensor(2.3046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  162  total  250 Loss:  tensor(2.3009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  163  total  250 Loss:  tensor(2.2966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  164  total  250 Loss:  tensor(2.2959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  165  total  250 Loss:  tensor(2.3049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  166  total  250 Loss:  tensor(2.3012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  167  total  250 Loss:  tensor(2.2982, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  168  total  250 Loss:  tensor(2.3015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  169  total  250 Loss:  tensor(2.3025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  170  total  250 Loss:  tensor(2.3022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  171  total  250 Loss:  tensor(2.3049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  172  total  250 Loss:  tensor(2.3009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  173  total  250 Loss:  tensor(2.3013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  174  total  250 Loss:  tensor(2.3036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  175  total  250 Loss:  tensor(2.3058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  176  total  250 Loss:  tensor(2.2991, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  177  total  250 Loss:  tensor(2.2985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  178  total  250 Loss:  tensor(2.3019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  179  total  250 Loss:  tensor(2.3012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  180  total  250 Loss:  tensor(2.3045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  181  total  250 Loss:  tensor(2.3031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  182  total  250 Loss:  tensor(2.3028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  183  total  250 Loss:  tensor(2.2977, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  184  total  250 Loss:  tensor(2.3007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  185  total  250 Loss:  tensor(2.3032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  186  total  250 Loss:  tensor(2.3066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  187  total  250 Loss:  tensor(2.3038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  188  total  250 Loss:  tensor(2.3058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  189  total  250 Loss:  tensor(2.3023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  190  total  250 Loss:  tensor(2.3044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  191  total  250 Loss:  tensor(2.3020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  192  total  250 Loss:  tensor(2.3037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  193  total  250 Loss:  tensor(2.3074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  194  total  250 Loss:  tensor(2.2996, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  195  total  250 Loss:  tensor(2.3007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  196  total  250 Loss:  tensor(2.3047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  197  total  250 Loss:  tensor(2.3020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  198  total  250 Loss:  tensor(2.3038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  199  total  250 Loss:  tensor(2.3035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  200  total  250 Loss:  tensor(2.3070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  201  total  250 Loss:  tensor(2.3016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  202  total  250 Loss:  tensor(2.3017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  203  total  250 Loss:  tensor(2.3044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  204  total  250 Loss:  tensor(2.3044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  205  total  250 Loss:  tensor(2.3035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  206  total  250 Loss:  tensor(2.2988, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  207  total  250 Loss:  tensor(2.3021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  208  total  250 Loss:  tensor(2.3063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  209  total  250 Loss:  tensor(2.2961, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  210  total  250 Loss:  tensor(2.3055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  211  total  250 Loss:  tensor(2.3032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  212  total  250 Loss:  tensor(2.3029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  213  total  250 Loss:  tensor(2.2988, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  214  total  250 Loss:  tensor(2.3002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  215  total  250 Loss:  tensor(2.3061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  216  total  250 Loss:  tensor(2.3055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  217  total  250 Loss:  tensor(2.3013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  218  total  250 Loss:  tensor(2.3057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  219  total  250 Loss:  tensor(2.3058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  220  total  250 Loss:  tensor(2.3065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  221  total  250 Loss:  tensor(2.3037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  222  total  250 Loss:  tensor(2.3037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  223  total  250 Loss:  tensor(2.2992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  224  total  250 Loss:  tensor(2.3020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  225  total  250 Loss:  tensor(2.3030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  226  total  250 Loss:  tensor(2.2965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  227  total  250 Loss:  tensor(2.2998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  228  total  250 Loss:  tensor(2.3016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  229  total  250 Loss:  tensor(2.3018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  230  total  250 Loss:  tensor(2.2984, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  231  total  250 Loss:  tensor(2.3059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  232  total  250 Loss:  tensor(2.3039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  233  total  250 Loss:  tensor(2.3001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  234  total  250 Loss:  tensor(2.3056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  235  total  250 Loss:  tensor(2.3021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  236  total  250 Loss:  tensor(2.3006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  237  total  250 Loss:  tensor(2.2990, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  238  total  250 Loss:  tensor(2.3094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  239  total  250 Loss:  tensor(2.3039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  240  total  250 Loss:  tensor(2.3033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  241  total  250 Loss:  tensor(2.3035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  242  total  250 Loss:  tensor(2.3062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  243  total  250 Loss:  tensor(2.3063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  244  total  250 Loss:  tensor(2.3006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  245  total  250 Loss:  tensor(2.2989, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  246  total  250 Loss:  tensor(2.2968, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  247  total  250 Loss:  tensor(2.2969, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  248  total  250 Loss:  tensor(2.3019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  3  Step:  249  total  250 Loss:  tensor(2.3038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  0  total  250 Loss:  tensor(2.3031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  1  total  250 Loss:  tensor(2.3014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  2  total  250 Loss:  tensor(2.3034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  3  total  250 Loss:  tensor(2.3024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  4  total  250 Loss:  tensor(2.3038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  5  total  250 Loss:  tensor(2.3029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  6  total  250 Loss:  tensor(2.3025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  7  total  250 Loss:  tensor(2.2984, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  8  total  250 Loss:  tensor(2.3026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  9  total  250 Loss:  tensor(2.3032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  10  total  250 Loss:  tensor(2.3019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  11  total  250 Loss:  tensor(2.3055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  12  total  250 Loss:  tensor(2.3055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  13  total  250 Loss:  tensor(2.3034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  14  total  250 Loss:  tensor(2.3030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  15  total  250 Loss:  tensor(2.3031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  16  total  250 Loss:  tensor(2.3014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  17  total  250 Loss:  tensor(2.3052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  18  total  250 Loss:  tensor(2.3062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  19  total  250 Loss:  tensor(2.3001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  20  total  250 Loss:  tensor(2.3003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  21  total  250 Loss:  tensor(2.3101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  22  total  250 Loss:  tensor(2.3021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  23  total  250 Loss:  tensor(2.3034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  24  total  250 Loss:  tensor(2.2979, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  25  total  250 Loss:  tensor(2.3034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  26  total  250 Loss:  tensor(2.2994, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  27  total  250 Loss:  tensor(2.3016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  28  total  250 Loss:  tensor(2.3016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  29  total  250 Loss:  tensor(2.3097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  30  total  250 Loss:  tensor(2.3033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  31  total  250 Loss:  tensor(2.3054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  32  total  250 Loss:  tensor(2.3046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  33  total  250 Loss:  tensor(2.2977, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  34  total  250 Loss:  tensor(2.3011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  35  total  250 Loss:  tensor(2.3056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  36  total  250 Loss:  tensor(2.3007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  37  total  250 Loss:  tensor(2.3085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  38  total  250 Loss:  tensor(2.3039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  39  total  250 Loss:  tensor(2.2986, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  40  total  250 Loss:  tensor(2.2988, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  41  total  250 Loss:  tensor(2.3061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  42  total  250 Loss:  tensor(2.3018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  43  total  250 Loss:  tensor(2.3054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  44  total  250 Loss:  tensor(2.3046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  45  total  250 Loss:  tensor(2.3055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  46  total  250 Loss:  tensor(2.3072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  47  total  250 Loss:  tensor(2.3091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  48  total  250 Loss:  tensor(2.3021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  49  total  250 Loss:  tensor(2.3047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  50  total  250 Loss:  tensor(2.3055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  51  total  250 Loss:  tensor(2.3044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  52  total  250 Loss:  tensor(2.3028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  53  total  250 Loss:  tensor(2.2993, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  54  total  250 Loss:  tensor(2.3003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  55  total  250 Loss:  tensor(2.3040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  56  total  250 Loss:  tensor(2.3008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  57  total  250 Loss:  tensor(2.3022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  58  total  250 Loss:  tensor(2.3045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  59  total  250 Loss:  tensor(2.3048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  60  total  250 Loss:  tensor(2.3027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  61  total  250 Loss:  tensor(2.3059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  62  total  250 Loss:  tensor(2.3010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  63  total  250 Loss:  tensor(2.3035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  64  total  250 Loss:  tensor(2.2983, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  65  total  250 Loss:  tensor(2.3069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  66  total  250 Loss:  tensor(2.3013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  67  total  250 Loss:  tensor(2.3013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  68  total  250 Loss:  tensor(2.3071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  69  total  250 Loss:  tensor(2.2982, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  70  total  250 Loss:  tensor(2.3061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  71  total  250 Loss:  tensor(2.3025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  72  total  250 Loss:  tensor(2.3042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  73  total  250 Loss:  tensor(2.3035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  74  total  250 Loss:  tensor(2.3030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  75  total  250 Loss:  tensor(2.3056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  76  total  250 Loss:  tensor(2.3014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  77  total  250 Loss:  tensor(2.2991, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  78  total  250 Loss:  tensor(2.2983, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  79  total  250 Loss:  tensor(2.3000, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  80  total  250 Loss:  tensor(2.3048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  81  total  250 Loss:  tensor(2.2985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  82  total  250 Loss:  tensor(2.3016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  83  total  250 Loss:  tensor(2.3013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  84  total  250 Loss:  tensor(2.2979, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  85  total  250 Loss:  tensor(2.2964, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  86  total  250 Loss:  tensor(2.3042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  87  total  250 Loss:  tensor(2.2988, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  88  total  250 Loss:  tensor(2.3043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  89  total  250 Loss:  tensor(2.3061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  90  total  250 Loss:  tensor(2.3001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  91  total  250 Loss:  tensor(2.3033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  92  total  250 Loss:  tensor(2.3036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  93  total  250 Loss:  tensor(2.2991, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  94  total  250 Loss:  tensor(2.2996, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  95  total  250 Loss:  tensor(2.3025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  96  total  250 Loss:  tensor(2.3045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  97  total  250 Loss:  tensor(2.3014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  98  total  250 Loss:  tensor(2.3030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  99  total  250 Loss:  tensor(2.3043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  100  total  250 Loss:  tensor(2.3032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  101  total  250 Loss:  tensor(2.3075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  102  total  250 Loss:  tensor(2.3005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  103  total  250 Loss:  tensor(2.3005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  104  total  250 Loss:  tensor(2.3024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  105  total  250 Loss:  tensor(2.3045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  106  total  250 Loss:  tensor(2.3056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  107  total  250 Loss:  tensor(2.2988, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  108  total  250 Loss:  tensor(2.3056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  109  total  250 Loss:  tensor(2.3023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  110  total  250 Loss:  tensor(2.3068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  111  total  250 Loss:  tensor(2.3076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  112  total  250 Loss:  tensor(2.3040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  113  total  250 Loss:  tensor(2.3011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  114  total  250 Loss:  tensor(2.3022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  115  total  250 Loss:  tensor(2.2983, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  116  total  250 Loss:  tensor(2.3045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  117  total  250 Loss:  tensor(2.3052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  118  total  250 Loss:  tensor(2.3076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  119  total  250 Loss:  tensor(2.3009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  120  total  250 Loss:  tensor(2.3031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  121  total  250 Loss:  tensor(2.2982, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  122  total  250 Loss:  tensor(2.2995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  123  total  250 Loss:  tensor(2.3057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  124  total  250 Loss:  tensor(2.3014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  125  total  250 Loss:  tensor(2.3041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  126  total  250 Loss:  tensor(2.3005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  127  total  250 Loss:  tensor(2.2999, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  128  total  250 Loss:  tensor(2.3056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  129  total  250 Loss:  tensor(2.3010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  130  total  250 Loss:  tensor(2.3042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  131  total  250 Loss:  tensor(2.3049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  132  total  250 Loss:  tensor(2.3050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  133  total  250 Loss:  tensor(2.2998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  134  total  250 Loss:  tensor(2.3031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  135  total  250 Loss:  tensor(2.3005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  136  total  250 Loss:  tensor(2.3113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  137  total  250 Loss:  tensor(2.2963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  138  total  250 Loss:  tensor(2.3021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  139  total  250 Loss:  tensor(2.3064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  140  total  250 Loss:  tensor(2.2986, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  141  total  250 Loss:  tensor(2.3025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  142  total  250 Loss:  tensor(2.3014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  143  total  250 Loss:  tensor(2.3002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  144  total  250 Loss:  tensor(2.3039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  145  total  250 Loss:  tensor(2.2983, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  146  total  250 Loss:  tensor(2.3023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  147  total  250 Loss:  tensor(2.3021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  148  total  250 Loss:  tensor(2.3031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  149  total  250 Loss:  tensor(2.3033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  150  total  250 Loss:  tensor(2.2999, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  151  total  250 Loss:  tensor(2.3052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  152  total  250 Loss:  tensor(2.3033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  153  total  250 Loss:  tensor(2.3037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  154  total  250 Loss:  tensor(2.3013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  155  total  250 Loss:  tensor(2.3036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  156  total  250 Loss:  tensor(2.3071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch:  4  Step:  157  total  250 Loss:  tensor(2.3055, device='cuda:0', grad_fn=<NllLossBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TD6mAMS7sUCY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}